# –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–æ–ª–∂–Ω—ã –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å sf_
# –∏ –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–ø–æ —É—Å–º–æ—Ç—Ä–µ–Ω–∏—é —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞)

import os
import io
from abc import ABC, abstractmethod
from langchain.docstore.document import Document as LangDocument

class sf_default:
    def __init__(self, file_path):
        self.file_path = file_path
    def separate_file(self):
        import os
        from langchain_community.document_loaders import PyPDFLoader
        from langchain_community.document_loaders import Docx2txtLoader
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )
        basename, extension = os.path.splitext(self.file_path)

        match extension:
            case ".docx":
                loader = Docx2txtLoader(self.file_path)
            case ".pdf":  
                loader = PyPDFLoader(self.file_path)
            case _:
                print(f"–î–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è {self.file_path}")
                return []

        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,)
        documents = text_splitter.split_documents(documents)

        return documents
    
class sf_500_100_podg:
    def __init__(self, file_path):
        self.file_path = file_path
    def separate_file(self):
        import os
        import re 
        from langchain_community.document_loaders import PyPDFLoader
        from langchain_community.document_loaders import Docx2txtLoader
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )
        basename, extension = os.path.splitext(self.file_path)

        match extension:
            case ".docx":
                loader = Docx2txtLoader(self.file_path)
            case ".pdf":  
                loader = PyPDFLoader(self.file_path)
            case _:
                print(f"–î–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è {self.file_path}")
                return []

        documents = loader.load()

        # pages = []
        # for doc in documents:
        #     cleaned_document = doc.replace('\n', '')
        #     pages.append(cleaned_document)

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100,)
        documents = text_splitter.split_documents(documents)

        return documents
    
class sf_add_keywords_512_chunk:
    def __init__(self, file_path):
        self.file_path = file_path
    def separate_file(self):
        import os
        import re 
        from langchain_community.document_loaders import PyPDFLoader
        from langchain_community.document_loaders import Docx2txtLoader
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )

        print ("start sf_add_keywords_512_chunk")

        basename, extension = os.path.splitext(self.file_path)

        match extension:
            case ".docx":
                loader = Docx2txtLoader(self.file_path)
            case ".pdf":  
                loader = PyPDFLoader(self.file_path)
            case _:
                print(f"–î–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è {self.file_path}")
                return []

        documents = loader.load()

        doc_c = get_keywords(documents)
        keywords = doc_c.get_keywords_def()
        print (keywords)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50,)
        chunks = text_splitter.split_documents(documents)
        enriched_chunks = [doc_c.enrich_chunk_with_additional_info(chunk, keywords) for chunk in chunks]

        print ("finish sf_add_keywords_512_chunk")
        return enriched_chunks

class get_keywords:
    
    from typing import List
    from langchain_ollama import OllamaLLM
    
    promt_category = {
        "–ó–∞–∫–æ–Ω –∏–ª–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∞–∫—Ç": (
            {"–ù–æ–º–µ—Ä –∑–∞–∫–æ–Ω–∞ –∏–ª–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–∫—Ç–∞ " : "–ö–∞–∫–æ–π –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞?",
             "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–æ–Ω–∞ –∏–ª–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–∫—Ç–∞ " : "–ö–∞–∫–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞?",
             "–î–∞—Ç–∞ –∑–∞–∫–æ–Ω–∞ –∏–ª–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–∫—Ç–∞ " : "–ö–∞–∫–∞—è –¥–∞—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞?"}
        ),
        "–î–æ–≥–æ–≤–æ—Ä": (
            {"–ù–æ–º–µ—Ä –¥–æ–≥–æ–≤–æ—Ä–∞ ":"–ö–∞–∫–æ–π –Ω–æ–º–µ—Ä –¥–æ–≥–æ–≤–æ—Ä–∞?",
             "–î–∞—Ç–∞ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ ":"–ö–∞–∫–∞—è –¥–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞?",
             "–î–æ–≥–æ–≤–æ—Ä –∑–∞–∫–ª—é—á–µ–Ω –º–µ–∂–¥—É ":" –ú–µ–∂–¥—É –∫–µ–º –∑–∞–∫–ª—é—á–µ–Ω –¥–æ–≥–æ–≤–æ—Ä?",
             "–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ ":"–ö–∞–∫–æ–π –ø—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞?"}
        ),
        "–ü–∏—Å—å–º–æ": (
            {"–ü–∏—Å—å–º–æ –æ—Ç ":"–ö—Ç–æ –Ω–∞–ø–∏—Å–∞–ª –ø–∏—Å—å–º–æ?",
            "–î–∞—Ç–∞ –ø–∏—Å—å–º–∞ ":"–ö–∞–∫–∞—è –¥–∞—Ç–∞ –ø–∏—Å—å–º–∞?",
            "–ü–æ–ª—É—á–∞—Ç–µ–ª—å –ø–∏—Å—å–º–∞ " : "–ö—Ç–æ –ø–æ–ª—É—á–∞—Ç–µ–ª—å –ø–∏—Å—å–º–∞?",
            "–¢–µ–º–∞ –ø–∏—Å—å–º–∞ " : "–ö–∞–∫–∞—è —Ç–µ–º–∞ –ø–∏—Å—å–º–∞ ?"}
        ),
        "–ö—É—Ä—Å–æ–≤–∞—è –∏–ª–∏ –¥–∏–ø–ª–æ–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞": (
            {"–¢–µ–º–∞ –∫—É—Ä—Å–æ–≤–æ–π –∏–ª–∏ –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã ":"–ö–∞–∫–∞—è —Ç–µ–º–∞ —Ä–∞–±–æ—Ç—ã?",
            "–ê–≤—Ç–æ—Ä –∫—É—Ä—Å–æ–≤–æ–π –∏–ª–∏ –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã ":"–ö—Ç–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª —Ä–∞–±–æ—Ç—É?"}
        ),
        "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏": (
            {"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ ":" –ö–∞–∫–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏?"}
        ),
        "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –∏–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–µ–∫—Ç": (
            {"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è ":"–ö–∞–∫–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞?",
            "–ù–æ–º–µ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è ":"–ö–∞–∫–æ–π –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞?",
            "–ê–≤—Ç–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è ": "–ö—Ç–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª –¥–æ–∫—É–º–µ–Ω—Ç?"}
        ),
        "–†–∞—Å—Å–∫–∞–∑ –∏–ª–∏ –ø–æ–≤–µ—Å—Ç—å": (
            {"–ê–≤—Ç–æ—Ä —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è ": "–ö—Ç–æ –∞–≤—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞?", 
            "–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è": "–ö–∞–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞?"}
        ),
        "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è": (
            {}
        )
    }
    X_char = 1000
    llm_class = OllamaLLM(model="deepseek-r1:latest", temperature = "0.1")
    llm_keywords = OllamaLLM(model="llama3:latest", temperature = "0.0")
    
    def __init__(self, documents: List[LangDocument]):
        self.documents = documents
    
    def remove_text_between_tags(self, text: str):
        start_tag = '<think>'
        end_tag = '</think>'

        result = []
        last_position = 0

        while True:
            start_index = text.find(start_tag, last_position)
            if start_index == -1:
                break

            result.append(text[last_position:start_index])

            end_index = text.find(end_tag, start_index + len(start_tag))
            if end_index == -1:
                break

            last_position = end_index + len(end_tag)

        result.append(text[last_position:])

        return ''.join(result)
    
        
    def clean_doc(self):
        """
        –ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –ø–µ—Ä–µ–≤–æ–¥–∞ —Å—Ç—Ä–æ–∫–∏ –≤ page_content —É –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        """
        for doc in self.documents:
            doc.page_content = doc.page_content.replace('\n', '')

    def get_X_characters(self) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç X —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—è page_content.
        """
#         result = ""
#         for doc in self.documents:
#             if len(result) >= self.X_char:
#                 break
#             result += doc.page_content[:self.X_char - len(result)]
#         return result
        result_start = ""
        result_end = ""

        # –°–æ–±–∏—Ä–∞–µ–º X —Å–∏–º–≤–æ–ª–æ–≤ —Å –Ω–∞—á–∞–ª–∞ —Å–ø–∏—Å–∫–∞
        for doc in self.documents:
            if len(result_start) >= self.X_char:
                break
            result_start += doc.page_content[:self.X_char - len(result_start)]

        # –°–æ–±–∏—Ä–∞–µ–º X —Å–∏–º–≤–æ–ª–æ–≤ —Å –∫–æ–Ω—Ü–∞ —Å–ø–∏—Å–∫–∞ (–Ω–∞—á–∏–Ω–∞—è —Å –∫–æ–Ω—Ü–∞)
        for doc in reversed(self.documents):
            if len(result_end) >= self.X_char:
                break
            result_end = doc.page_content[-(self.X_char - len(result_end)):] + result_end

        return result_start + result_end

    def define_document_type(self, doc_for_context: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        """
        category_str = ""

        for category, promt in self.promt_category.items():
            category_str = category_str + category + ", "
        promt_category = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç: –º—ã –ø—Ä–æ–≤–æ–¥–∏–º —Ä–∞–±–æ—Ç—ã –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –∫ –∫–∞–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ç–µ–∫—Å—Ç
        –†–æ–ª—å: —Ç–≤–æ—è —Ä–æ–ª—å –ø–æ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –∫ –∫–∞–∫–æ–π –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç 
        –ó–∞–¥–∞—á–∞: –¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç {doc_for_context} —Ç—ã –¥–æ–ª–∂–µ–Ω –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫ –∫–∞–∫–æ–π  
        –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ —Å–ø–∏—Å–∫–∞ –æ–Ω –æ—Ç–Ω–æ—Å–∏—Ç—Å—è, —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {category_str}. 
        –ö—Ä–∏—Ç–µ—Ä–∏–∏ –ö–∞—á–µ—Å—Ç–≤–∞: –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ—á–Ω–æ –æ–¥–Ω—É –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –Ω–µ–ª—å–∑—è –º–µ–Ω—è—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞, –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        llm_response = self.llm_class.invoke(promt_category)
        return self.remove_text_between_tags(llm_response)

            
    def find_category(self, text: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–≤–∞—Ä—è promt_category.
        """
        for category in self.promt_category.keys():
            if category.lower() in text.lower():
                return category
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è"
    
    def return_promt_find_keywords(self, doc_type: str) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–º—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
        """
        promt_ = ""
        dictionary = self.promt_category
        if doc_type in dictionary:
            promt_ =  dictionary[doc_type]
        
        return promt_
    
    def find_keywords(self, input_dic, doc_char: str) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.
        """ 
        promt_keyword = "–í—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –í—ã –æ—Ç–≤–µ—á–∞–µ—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ: {self.data}. –û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –Ω–∞ —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å: {self.prompt} "
        promt_keyword = promt_keyword.replace("{self.data}", doc_char)
            
        #parts = input_str.split(";")  # –†–∞–∑–¥–µ–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ ";"
        modified_parts = []  # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —á–∞—Å—Ç–µ–π

        for key in input_dic:
            promt_llm = promt_keyword.replace("{self.prompt}", input_dic[key])
            llm_response = self.llm_keywords.invoke(promt_llm)
            modified_parts.append(key + " " + llm_response)  

        return ";".join(modified_parts)  # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å—Ç—Ä–æ–∫—É


        #llm_response = self.llm_pd.invoke(promt_keyword)
        #return self.remove_text_between_tags(llm_response)
    
    def add_keywords(self, additional_info: str):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –≤ page_content –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ–¥ –∫–ª—é—á–æ–º 'keywords'.
        """
        for doc in self.documents:
            if 'page_content' not in doc.__dict__:
                doc.page_content = ""
            doc.page_content += f"\n\n{additional_info}"
    
    def get_keywords_def(self):
        self.clean_doc()
        doc_char = self.get_X_characters()
        doc_type_llm = self.define_document_type(doc_char)
        doc_type = self.find_category(doc_type_llm)
        promt_key = self.return_promt_find_keywords(doc_type)
        return self.find_keywords(promt_key, doc_char)
    
    def enrich_chunk_with_additional_info(self, doc, additional_text):
        """–î–æ–±–∞–≤–ª—è–µ—Ç additional_info –≤ —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞, –Ω–æ –Ω–µ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        enriched_content = f"{additional_text}\n\n{doc.page_content}"  # üëà –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ `additional_info`
        return LangDocument(page_content=enriched_content, metadata=doc.metadata)
    