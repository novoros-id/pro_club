import pandas as pd
pd.set_option('future.no_silent_downcasting', True)

import os
from abc import ABC, abstractmethod
from langchain.docstore.document import Document as LangDocument

class sf_default:
    def __init__(self, file_path):
        self.file_path = file_path
    def separate_file(self):
        from langchain_community.document_loaders import PyPDFLoader
        from langchain_community.document_loaders import Docx2txtLoader
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )
        basename, extension = os.path.splitext(self.file_path)

        match extension:
            case ".docx":
                loader = Docx2txtLoader(self.file_path)
            case ".pdf":  
                loader = PyPDFLoader(self.file_path)
            case _:
                print(f"–î–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è {self.file_path}")
                return []

        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200,)
        documents = text_splitter.split_documents(documents)     

        return documents
    
class sf_DataProcessing:
    def __init__(self, file_path):
        self.file_path = file_path
    def separate_file(self):
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )

        loader = sfDocumentLoaderFactory.create_loader(self.file_path) 
        documents = loader.load_documents() 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50) 
        documents = text_splitter.split_documents(documents)
        for i, doc in enumerate(documents[:3]):
            print(f"–ß–∞–Ω–∫ {i}: {doc.page_content[:500]}")

        return documents

class sf_add_keywords_512_chunk:
    def __init__(self, file_path):
        self.file_path = file_path
    def separate_file(self):
        import os
        import re 
        from langchain_community.document_loaders import PyPDFLoader
        from langchain_community.document_loaders import Docx2txtLoader
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )
    
        # —á–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        basename, extension = os.path.splitext(self.file_path)
        match extension:
            case ".docx":
                loader = Docx2txtLoader(self.file_path)
            case ".pdf":  
                loader = PyPDFLoader(self.file_path)
            case _:
                print(f"–î–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è {self.file_path}")
                return []
        documents = loader.load()

        # –û–±—ä—è–≤–ª—è–µ–º –∫–ª–∞—Å—Å
        doc_c = get_keywords(documents)
        # –Ω–∞—Ö–æ–¥–∏–º —Å–ª–æ–≤–∞
        # –í–ê–ñ–ù–û! —Å–ª–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è —á–µ—Ä–µ–∑ —Å–µ—Ç—å, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ  —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Ç—å deepseek-r1:latest
        # –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç—å llm_class –∏ llm_keywords
        keywords = doc_c.get_keywords_def()
        # –≤ keywords —É –Ω–∞—Å —Ö—Ä–∞–Ω—è—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        print (keywords)

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100,)
        chunks = text_splitter.split_documents(documents)

        # –≤ chunk –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π –∫–ª–∞—Å—Å, –º—ã –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        enriched_chunks = [doc_c.enrich_chunk_with_additional_info(chunk, keywords) for chunk in chunks]

        # –∏ –≤–æ–∑—Ä–∞—â–∞–µ–º —ç—Ç–æ—Ç —á–∞–Ω–∫
        return enriched_chunks

class sf_DataProcessing_keywords_512_chunk:
    def __init__(self, file_path):
        self.file_path = file_path
    def separate_file(self):
        import os
        import re 
        from langchain_community.document_loaders import PyPDFLoader
        from langchain_community.document_loaders import Docx2txtLoader
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )
    
        # —á–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )

        loader = sfDocumentLoaderFactory.create_loader(self.file_path) 
        documents = loader.load_documents() 

        # –û–±—ä—è–≤–ª—è–µ–º –∫–ª–∞—Å—Å
        doc_c = get_keywords(documents)
        # –Ω–∞—Ö–æ–¥–∏–º —Å–ª–æ–≤–∞
        # –í–ê–ñ–ù–û! —Å–ª–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è —á–µ—Ä–µ–∑ —Å–µ—Ç—å, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ  —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Ç—å deepseek-r1:latest
        # –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç—å llm_class –∏ llm_keywords
        keywords = doc_c.get_keywords_def()
        # –≤ keywords —É –Ω–∞—Å —Ö—Ä–∞–Ω—è—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        print (keywords)

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100,)
        chunks = text_splitter.split_documents(documents)

        # –≤ chunk –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π –∫–ª–∞—Å—Å, –º—ã –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        enriched_chunks = [doc_c.enrich_chunk_with_additional_info(chunk, keywords) for chunk in chunks]

        # –∏ –≤–æ–∑—Ä–∞—â–∞–µ–º —ç—Ç–æ—Ç —á–∞–Ω–∫
        return enriched_chunks
    
class sf_DataProcessing_keywords_512_chunk_and_Tables:
    def __init__(self, file_path):
        self.file_path = file_path
    def separate_file(self):
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )
    
        # —á–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        from langchain.text_splitter import (
            RecursiveCharacterTextSplitter,
        )

        loader = sfDocumentLoaderFactory.create_loader(self.file_path) 
        documents = loader.load_documents() 

        # –û–±—ä—è–≤–ª—è–µ–º –∫–ª–∞—Å—Å
        doc_c = get_keywords(documents)
        # –Ω–∞—Ö–æ–¥–∏–º —Å–ª–æ–≤–∞
        # –í–ê–ñ–ù–û! —Å–ª–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è —á–µ—Ä–µ–∑ —Å–µ—Ç—å, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ  —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Ç—å deepseek-r1:latest
        # –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç—å llm_class –∏ llm_keywords
        keywords = doc_c.get_keywords_def()
        # –≤ keywords —É –Ω–∞—Å —Ö—Ä–∞–Ω—è—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        print (keywords)

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50,)
        chunks = text_splitter.split_documents(documents)

        # –≤ chunk –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π –∫–ª–∞—Å—Å, –º—ã –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        enriched_chunks = [doc_c.enrich_chunk_with_additional_info(chunk, keywords) for chunk in chunks]

        # –∏ –≤–æ–∑—Ä–∞—â–∞–µ–º —ç—Ç–æ—Ç —á–∞–Ω–∫
        return enriched_chunks

# –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
class sfBaseDocumentLoader(ABC):
    @abstractmethod
    def load_documents(self) -> list:
        # –ú–µ—Ç–æ–¥ –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ LangDocument
        pass

# –ö–ª–∞—Å—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞
class sfFileTypeDetector:
    @staticmethod
    def get_file_type(file_path: str) -> str:
        _, ext = os.path.splitext(file_path)
        return ext.lower()

# –ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX, –≤–∫–ª—é—á–∞—è —Å–ø–∏—Å–∫–∏, –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –æ–±—ã—á–Ω—ã–µ –∞–±–∑–∞—Ü—ã
class sfDOCXTextExtractor:
    def __init__(self, file_path: str):
        self.file_path = file_path
    
    def extract_text(self) -> str:
        from docx import Document as DocxDocument
        try:
            doc = DocxDocument(self.file_path)
            text_data = []
            for para in doc.paragraphs:
                if para.style.name.startswith("Heading"): # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≥–∞–ª–æ–≤–∫–æ–≤
                    text_data.append(f"\n**{para.text.strip()}**")
                else:
                    text_data.append(para.text.strip())
            return "\n".join(text_data)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
            return ""

# –ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü –∏–∑ DOCX
class sfDOCXTableExtractor:
    def __init__(self, file_path: str):
        self.file_path = file_path

    def extract_tables(self) -> str:
        from docx import Document as DocxDocument
        try:
            doc = DocxDocument(self.file_path)
            tables_text = []
            for table in doc.tables:
                rows = []
                for row in table.rows:
                    cells = [cell.text.strip() for cell in row.cells]
                    rows.append("\t".join(cells))
                tables_text.append("\n".join(rows))
            return "\n\n".join(tables_text)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü: {e}")
            return ""

# # –ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ DOCX –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è OCR        
# class sfDOCXImageExtractor:
#     def __init__(self, file_path: str):
#         self.file_path = file_path

#     def extract_images_text(self) -> str:
#         import zipfile
#         import pytesseract
#         from PIL import Image

#         ocr_text = []
#         try:
#             with zipfile.ZipFile(self.file_path) as docx_zip:
#                 for file in docx_zip.namelist():
#                     if file.startswith("word/media/"):
#                         try:
#                             image_data = docx_zip.read(file)
#                             image = Image.open(io.BytesIO(image_data))
#                             if image.format == 'WMF':
#                                 image = image.convert('RGB') # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º WMF –≤ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π pytesseract —Ñ–æ—Ä–º–∞—Ç 
#                             text = pytesseract.image_to_string(image).strip()
#                             if text:
#                                 ocr_text.append(text)
#                         except Exception as img_e:
#                             print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {file}: {img_e}")
#         except Exception as e:
#             print (f"–û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è DOCX –∫–∞–∫ zip-–∞—Ä—Ö–∏–≤–∞: {e}")
#         return "\n\n".join(ocr_text)
            
# –§–∞–±—Ä–∏–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ DOCX (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, —Ç–∞–±–ª–∏—Ü, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
class sfDOCXLoader(sfBaseDocumentLoader):
    # def __init__(self, file_path: str, loader_type: str):
    #     self.file_path = file_path
    #     self.loader_type = loader_type
    #     self.text_extractor = sfDOCXTextExtractor(file_path)
    #     self.table_extractor = sfDOCXTableExtractor(file_path)
    #     # self.image_extractor = sfDOCXImageExtractor(file_path)

    # def clean_text(self, text:str) -> str:
    #     import re
    #     text = re.sub(r'\s+',' ', text)
    #     return text.strip()

    # def load_documents(self):
    #     documents = []
    #     metadata = {"source": self.file_path}

    #     # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    #     text = self.text_extractor.extract_text()
    #     if text.strip():
    #         cleaned_text = self.clean_text(text)
    #         documents.append(LangDocument(page_content=cleaned_text, metadata=metadata))

    #     # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
    #     table_text = self.table_extractor.extract_tables()
    #     if table_text.strip():
    #         cleaned_table_text = self.clean_text(table_text)
    #         documents.append(LangDocument(page_content=cleaned_table_text, metadata=metadata))

    #     # # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    #     # images_text = self.image_extractor.extract_images_text()
    #     # if images_text.strip():
    #     #     documents.append(LangDocument(page_content=images_text))

    #     return documents


    def __init__(self, file_path: str, loader_type: str):
        self.file_path = file_path
        self.loader_type = loader_type

    def clean_text(self, text:str) -> str:
        import re
        text = re.sub(r'\s+',' ', text)
        return text.strip()

    def load_documents(self):
        from langchain_community.document_loaders import UnstructuredWordDocumentLoader
        from langchain.docstore.document import Document as LangDocument

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º UnstructuredWordDocumentLoader –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ .docx
        loader = UnstructuredWordDocumentLoader(self.file_path)
        raw_documents = loader.load()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ LangDocument
        documents = [
            LangDocument(page_content=doc.page_content, metadata={"source": self.file_path})
            for doc in raw_documents
        ]
        return documents

#  –ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF
class sfPDFTextExtractor:
    def __init__(self, file_path: str):
        self.file_path = file_path

    # def analyze_page(self, page):
    #     analysis = {"contains_text": False, "contains_tables": False}
    #     try:
    #         page_dict = page.get_text("dict")
    #         for block in page_dict.get("blocks", []):
    #             if "text" in block and block["text"].strip():
    #                 analysis["contains_text"] = True
    #             # if "image" in block:
    #             #     analysis["contains_images"] = True
    #             if "lines" in block:
    #                 # print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page.number + 1}: –ù–∞–π–¥–µ–Ω–æ {len(block['lines'])} –ª–∏–Ω–∏–π")
    #                 analysis["contains_tables"] = True
    #     except Exception as e:
    #         print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
    #     return analysis

    def analyze_page(self, page):
        analysis = {"contains_text": False, "contains_tables": False}
        try:
            tables = page.find_tables()
            if tables.tables:  # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Ç–∞–±–ª–∏—Ü—ã
                analysis["contains_tables"] = True
            else:
                text = page.get_text("text").strip()
                if text:
                    analysis["contains_text"] = True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return analysis

    def extract_text(self, page):
        text = ""
        analysis = self.analyze_page(page)
        if analysis["contains_text"]:
            try:
                text = page.get_text("text").strip()
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        # elif analysis["contains_images"]:
        #     try:
        #         pix = page.get_pixmap()
        #         from PIL import Image
        #         image = Image.open(io.BytesIO(pix.tobytes()))
        #         import pytesseract
        #         text = pytesseract.image_to_string(image).strip()
        #     except Exception as e:
        #         print(f"–û—à–∏–±–∫–∞ OCR: {e}")
        return text

# –ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü –∏–∑ PDF
class sfPDFTableExtractor:
    def __init__(self, file_path: str, text_extractor: sfPDFTextExtractor = None):
        self.file_path = file_path
        self.text_extrarctor = text_extractor if text_extractor is not None else sfPDFTextExtractor(path)
        self._cache = {} # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    def try_extract_table(self, page_num, page):
        # –ü—Ä–æ–≤–µ—Ä–∏–º –µ—Å—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à–µ
        if page_num in self._cache:
            return self._cache[page_num]

        import camelot
        table_text_candidates = {}

        try:
            page_dict = page.get_text("dict")
            contains_lines = any("lines" in block for block in page_dict.get("blocks", []))
            flavors = ["lattice", "stream", "hybrid"] if contains_lines else ["stream"]

            for flavor in flavors:
                try:
                    tables = camelot.read_pdf(self.file_path, pages=str(page_num), flavor=flavor)
                    if tables.n > 0:
                        quality = sum(len(table.df) for table in tables)
                        table_text = "\n".join([table.df.to_csv(index=False) for table in tables])
                        table_text_candidates[flavor] = (quality, table_text)
                        # print(f"Flavor '{flavor}' –∏–∑–≤–ª–µ–∫ {tables.n} —Ç–∞–±–ª–∏—Ü –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num} —Å –∫–∞—á–µ—Å—Ç–≤–æ–º {quality}.")
                except Exception as e:
                    print(f"Flavor '{flavor}' –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {e}")

            if not table_text_candidates:
                print(f"–¢–∞–±–ª–∏—Ü—ã –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}.")
                self._cache[page_num] = ""
                return ""
                        
            best_flavor = max(table_text_candidates, key=lambda f: table_text_candidates[f][0])
            best_quality, best_text = table_text_candidates[best_flavor]
            print(f"–í—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º '{best_flavor}' –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} —Å –∫–∞—á–µ—Å—Ç–≤–æ–º {best_quality}.")
            self._cache[page_num] = best_text
            return best_text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü: {e}")
            return ""

    def extract_tables(self):
        extracted_tables = []
        metadata = {"source": self.file_path}

        import fitz  # PyMuPDF
        try:
            doc = fitz.open(self.file_path)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è PDF: {e}")
            return ""
        
        for page in doc:
            page_num = page.number + 1
            analysis = sfPDFTextExtractor(self.file_path).analyze_page(page)
            if analysis["contains_tables"]:
                table_text = self.try_extract_table(page_num, page)
                if table_text:
                    extracted_tables.append(table_text)
        return "\n\n".join(extracted_tables) if extracted_tables else "", metadata

# –ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ PDF, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ç–µ–∫—Å—Ç –∏ —Ç–∞–±–ª–∏—Ü—ã
class sfPDFLoader(sfBaseDocumentLoader):
    def __init__(self, file_path: str, loader_type: str):
        self.file_path = file_path
        self.loader_type = loader_type
        self.text_extractor = sfPDFTextExtractor(file_path)
        self.table_extractor = sfPDFTableExtractor(file_path, text_extractor=self.text_extractor)

    def load_documents(self):
        if self.loader_type == "py":
            from langchain_community.document_loaders import PyPDFLoader
            loader = PyPDFLoader(self.file_path)
        elif self.loader_type == "unstructured":
            from langchain_community.document_loaders import UnstructuredPDFLoader
            loader = UnstructuredPDFLoader(self.file_path)
        elif self.loader_type == "plumber":
            from langchain_community.document_loaders import PDFMinerLoader
            loader = PDFMinerLoader(self.file_path)        
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–ª—è PDF: {self.loader_type}")
        
        documents = loader.load()
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
        tables_text, table_metadata = self.table_extractor.extract_tables()
        if tables_text.strip():
            documents.append(LangDocument(page_content=tables_text, metadata=table_metadata))
        return documents

class get_keywords:
    
    from typing import List
    from langchain_ollama import OllamaLLM
    
    promt_category = {
        "–ó–∞–∫–æ–Ω –∏–ª–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∞–∫—Ç": (
            {"–ù–æ–º–µ—Ä –∑–∞–∫–æ–Ω–∞ –∏–ª–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–∫—Ç–∞ " : "–ö–∞–∫–æ–π –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞?",
             "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–æ–Ω–∞ –∏–ª–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–∫—Ç–∞ " : "–ö–∞–∫–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞?",
             "–î–∞—Ç–∞ –∑–∞–∫–æ–Ω–∞ –∏–ª–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–∫—Ç–∞ " : "–ö–∞–∫–∞—è –¥–∞—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞?"}
        ),
        "–î–æ–≥–æ–≤–æ—Ä": (
            {"–ù–æ–º–µ—Ä –¥–æ–≥–æ–≤–æ—Ä–∞ ":"–ö–∞–∫–æ–π –Ω–æ–º–µ—Ä –¥–æ–≥–æ–≤–æ—Ä–∞?",
             "–î–∞—Ç–∞ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ ":"–ö–∞–∫–∞—è –¥–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞?",
             "–î–æ–≥–æ–≤–æ—Ä –∑–∞–∫–ª—é—á–µ–Ω –º–µ–∂–¥—É ":" –ú–µ–∂–¥—É –∫–µ–º –∑–∞–∫–ª—é—á–µ–Ω –¥–æ–≥–æ–≤–æ—Ä?",
             "–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ ":"–ö–∞–∫–æ–π –ø—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞?"}
        ),
        "–ü–∏—Å—å–º–æ": (
            {"–ü–∏—Å—å–º–æ –æ—Ç ":"–ö—Ç–æ –Ω–∞–ø–∏—Å–∞–ª –ø–∏—Å—å–º–æ?",
            "–î–∞—Ç–∞ –ø–∏—Å—å–º–∞ ":"–ö–∞–∫–∞—è –¥–∞—Ç–∞ –ø–∏—Å—å–º–∞?",
            "–ü–æ–ª—É—á–∞—Ç–µ–ª—å –ø–∏—Å—å–º–∞ " : "–ö—Ç–æ –ø–æ–ª—É—á–∞—Ç–µ–ª—å –ø–∏—Å—å–º–∞?",
            "–¢–µ–º–∞ –ø–∏—Å—å–º–∞ " : "–ö–∞–∫–∞—è —Ç–µ–º–∞ –ø–∏—Å—å–º–∞ ?"}
        ),
        "–ö—É—Ä—Å–æ–≤–∞—è –∏–ª–∏ –¥–∏–ø–ª–æ–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞": (
            {"–¢–µ–º–∞ –∫—É—Ä—Å–æ–≤–æ–π –∏–ª–∏ –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã ":"–ö–∞–∫–∞—è —Ç–µ–º–∞ —Ä–∞–±–æ—Ç—ã?",
            "–ê–≤—Ç–æ—Ä –∫—É—Ä—Å–æ–≤–æ–π –∏–ª–∏ –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã ":"–ö—Ç–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª —Ä–∞–±–æ—Ç—É?"}
        ),
        "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏": (
            {"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ ":" –ö–∞–∫–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏?"}
        ),
        "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –∏–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–µ–∫—Ç": (
            {"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è ":"–ö–∞–∫–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞?",
            "–ù–æ–º–µ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è ":"–ö–∞–∫–æ–π –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞?",
            "–ê–≤—Ç–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è ": "–ö—Ç–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª –¥–æ–∫—É–º–µ–Ω—Ç?"}
        ),
        "–†–∞—Å—Å–∫–∞–∑ –∏–ª–∏ –ø–æ–≤–µ—Å—Ç—å": (
            {"–ê–≤—Ç–æ—Ä —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è ": "–ö—Ç–æ –∞–≤—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞?", 
            "–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è": "–ö–∞–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞?"}
        ),
        "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è": (
            {}
        )
    }
    X_char = 1000
    llm_class = OllamaLLM(model="deepseek-r1:latest", temperature = "0.1")
    llm_keywords = OllamaLLM(model="llama3:latest", temperature = "0.0")
    
    def __init__(self, documents: List[LangDocument]):
        self.documents = documents
    
    def remove_text_between_tags(self, text: str):
        start_tag = '<think>'
        end_tag = '</think>'

        result = []
        last_position = 0

        while True:
            start_index = text.find(start_tag, last_position)
            if start_index == -1:
                break

            result.append(text[last_position:start_index])

            end_index = text.find(end_tag, start_index + len(start_tag))
            if end_index == -1:
                break

            last_position = end_index + len(end_tag)

        result.append(text[last_position:])

        return ''.join(result)
    
        
    def clean_doc(self):
        """
        –ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –ø–µ—Ä–µ–≤–æ–¥–∞ —Å—Ç—Ä–æ–∫–∏ –≤ page_content —É –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        """
        for doc in self.documents:
            doc.page_content = doc.page_content.replace('\n', '')

    def get_X_characters(self) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç X —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—è page_content.
        """
#         result = ""
#         for doc in self.documents:
#             if len(result) >= self.X_char:
#                 break
#             result += doc.page_content[:self.X_char - len(result)]
#         return result
        result_start = ""
        result_end = ""

        # –°–æ–±–∏—Ä–∞–µ–º X —Å–∏–º–≤–æ–ª–æ–≤ —Å –Ω–∞—á–∞–ª–∞ —Å–ø–∏—Å–∫–∞
        for doc in self.documents:
            if len(result_start) >= self.X_char:
                break
            result_start += doc.page_content[:self.X_char - len(result_start)]

        # –°–æ–±–∏—Ä–∞–µ–º X —Å–∏–º–≤–æ–ª–æ–≤ —Å –∫–æ–Ω—Ü–∞ —Å–ø–∏—Å–∫–∞ (–Ω–∞—á–∏–Ω–∞—è —Å –∫–æ–Ω—Ü–∞)
        for doc in reversed(self.documents):
            if len(result_end) >= self.X_char:
                break
            result_end = doc.page_content[-(self.X_char - len(result_end)):] + result_end

        return result_start + result_end

    def define_document_type(self, doc_for_context: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        """
        category_str = ""

        for category, promt in self.promt_category.items():
            category_str = category_str + category + ", "
        promt_category = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç: –º—ã –ø—Ä–æ–≤–æ–¥–∏–º —Ä–∞–±–æ—Ç—ã –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –∫ –∫–∞–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ç–µ–∫—Å—Ç
        –†–æ–ª—å: —Ç–≤–æ—è —Ä–æ–ª—å –ø–æ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –∫ –∫–∞–∫–æ–π –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç 
        –ó–∞–¥–∞—á–∞: –¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç {doc_for_context} —Ç—ã –¥–æ–ª–∂–µ–Ω –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫ –∫–∞–∫–æ–π  
        –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ —Å–ø–∏—Å–∫–∞ –æ–Ω –æ—Ç–Ω–æ—Å–∏—Ç—Å—è, —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {category_str}. 
        –ö—Ä–∏—Ç–µ—Ä–∏–∏ –ö–∞—á–µ—Å—Ç–≤–∞: –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ—á–Ω–æ –æ–¥–Ω—É –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –Ω–µ–ª—å–∑—è –º–µ–Ω—è—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞, –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        llm_response = self.llm_class.invoke(promt_category)
        return self.remove_text_between_tags(llm_response)

            
    def find_category(self, text: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–≤–∞—Ä—è promt_category.
        """
        for category in self.promt_category.keys():
            if category.lower() in text.lower():
                return category
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è"
    
    def return_promt_find_keywords(self, doc_type: str) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–º—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
        """
        promt_ = ""
        dictionary = self.promt_category
        if doc_type in dictionary:
            promt_ =  dictionary[doc_type]
        
        return promt_
    
    def find_keywords(self, input_dic, doc_char: str) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.
        """ 
        promt_keyword = "–í—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –í—ã –æ—Ç–≤–µ—á–∞–µ—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ: {self.data}. –û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –Ω–∞ —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å: {self.prompt} "
        promt_keyword = promt_keyword.replace("{self.data}", doc_char)
            
        #parts = input_str.split(";")  # –†–∞–∑–¥–µ–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ ";"
        modified_parts = []  # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —á–∞—Å—Ç–µ–π

        for key in input_dic:
            promt_llm = promt_keyword.replace("{self.prompt}", input_dic[key])
            llm_response = self.llm_keywords.invoke(promt_llm)
            modified_parts.append(key + " " + llm_response)  

        return ";".join(modified_parts)  # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å—Ç—Ä–æ–∫—É


        #llm_response = self.llm_pd.invoke(promt_keyword)
        #return self.remove_text_between_tags(llm_response)
    
    def add_keywords(self, additional_info: str):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –≤ page_content –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ–¥ –∫–ª—é—á–æ–º 'keywords'.
        """
        for doc in self.documents:
            if 'page_content' not in doc.__dict__:
                doc.page_content = ""
            doc.page_content += f"\n\n{additional_info}"
    
    def get_keywords_def(self):
        self.clean_doc()
        doc_char = self.get_X_characters()
        doc_type_llm = self.define_document_type(doc_char)
        doc_type = self.find_category(doc_type_llm)
        promt_key = self.return_promt_find_keywords(doc_type)
        return self.find_keywords(promt_key, doc_char)
    
    def enrich_chunk_with_additional_info(self, doc, additional_text):
        """–î–æ–±–∞–≤–ª—è–µ—Ç additional_info –≤ —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞, –Ω–æ –Ω–µ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        enriched_content = f"{additional_text}\n\n{doc.page_content}"  # üëà –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ `additional_info`
        return LangDocument(page_content=enriched_content, metadata=doc.metadata)

# –§–∞–±—Ä–∏–∫–∞ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω—É–∂–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫
class sfDocumentLoaderFactory:
    @staticmethod
    def create_loader(file_path: str):
        ext = sfFileTypeDetector.get_file_type(file_path)
        if ext == ".pdf":
            return sfPDFLoader(file_path, loader_type="py")
        elif ext == ".docx":
            return sfDOCXLoader(file_path, loader_type="python-docx")
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {ext}")

#  –ö–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: –∑–∞–≥—Ä—É–∑–∫–∞ -> —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
class sfDocumentProcessingPipeline:
    def __init__(self, file_path: str):
        self.loader = sfDocumentLoaderFactory.create_loader(file_path)
        # self.splitter = sfTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    
    def separate_file(self):
        documents = self.loader.load_documents()
        return self.splitter.split(documents)